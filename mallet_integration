#user:siddhant_08
#created: 20/03/2016

# code to generate a vector of the most frequent words in a corpus using tm and stylo
# also performs cluster analysis using stylo
# topic modelling via mallet using LDA algorithm
# SnowballC enables parallel computing in the tm_map function for stemming.
# tm_map calls functions of the 'parallel' package.
# also Uses mclapply under the hood.
# 'parallel'does not work for windows so only a single core will be used

if(!require(tm) | !require(SnowballC) | !require(stylo) | !require(mallet) | !require(parallel)) {
	install.packages(c('tm','SnowballC','stylo','mallet','parallel'))
}  

require(tm)
require(SnowballC)
require(stylo)
require(mallet)
require(parallel)
 
heritage<- VCorpus(DirSource("looted_heritage_reports"))
 
heritage <- tm_map(heritage, content_transformer(tolower))
 
heritage <- tm_map(heritage,removePunctuation)
 
heritage<- tm_map(heritage,removeNumbers)
#remove stopwords and '>'
 
stp_wrds<- stopwords("english")
heritage<-tm_map(heritage,removeWords,c(stp_wrds,">"))
 
#remove whitespace generated after removing stopwords
heritage <- tm_map(heritage, stripWhitespace)
 
#stemming
heritage <- tm_map(heritage, stemDocument)
 
# generates the document term matrix for the corpus
dtmb<-DocumentTermMatrix(heritage)
#str(docs_list)
 
# taking only those words which are in atleast 10% of texts
# taking maximum 100 words
# input is the frequencies matrix generated in tm
 
pre_process2<-stylo(gui=FALSE,
                   analysis.type="CA",
                   frequencies = as.matrix(dtmb),
                   corpus.lang="english",
                   mfw.min=100,mfw.max=100,
                   culling.min =10,culling.max=10,
                   write.jpg.file=TRUE,
                   save.distance.tables = TRUE,
                   save.analyzed.features = TRUE,
                   save.analyzed.freqs = TRUE)
 
processed2<-pre_process2$features.actually.used
#list of the mfw
vocab2<-as.vector(processed)
 
# now getting required format for mallet to work with
# a character vector of text in documents and a vector containing document ids
 
#tokenizing
tok_hh<-txt.to.words.ext(heritage,language = "English.all")
#removing pronouns
tok_hh_no_pro<- delete.stop.words(tok_hh,stop.words=stylo.pronouns(language="English"))
 
# extracting document IDs
id2<-names(tok_hh_no_pro)
 
num_docs2<-length(tok_hh_no_pro)
her_text<-vector()
 
# forming the text to pass to mallet
# too bad it can't take DocumentTermMatrix as input
# like the topicmodels package
# Might be a good task to work on
 
for(i in 1: num_docs2){
  her_text<-rbind(her_text,paste(tok_hh_no_pro[[i]],collapse = ' '))
}
#removing NAs
her_text<-her_text[!is.na(her_text)]
 
# initialising the topic model.
# 5 topics only because the sample corpus
# is related to just one subject hence
# overlap between topics is very high.
# lots of the words are in many topics
 
topic.model<-MalletLDA(num.topics=5)
 
# "my.txt" is a file containing stopwords
# this has to be specified as a file
# at the moment mallet can't take a list of stopwords
# I'll be looking to change this during the course of the project
# Since I had already removed stopwords earlier I created a empty file
 
mallet.instances<-mallet.import(id2,her_text,"my.txt")
topic.model$loadDocuments(mallet.instances)
 
#matrix of topic weights for every document
doc_tops2<- mallet.doc.topics(topic.model,normalized=T,smoothed=T)
 
# matrix of word weights for topics
word_tops2<-mallet.topic.words(topic.model,normalized=T,smoothed=T)
 
# top 10 words in topic 2
topwords2<-mallet.top.words(topic.model,word_tops2[2,],10)
topwords2
 
# top 10 words in topic 3
topwords3<-mallet.top.words(topic.model,word_tops2[3,],10)
topwords3
